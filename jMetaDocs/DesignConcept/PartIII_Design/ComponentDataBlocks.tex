%-----------------------------------------------------------------------------------------------
%		\COMPdataPartManagement{} Design
%-----------------------------------------------------------------------------------------------

\section{\COMPdataPartManagement{} Design}
\label{sec:COMPdataBlockManagementDesign}

In this section, the design of the component \COMPdataPartManagement{} is described. Basic task of the component is to implement the actual generic parsing of data of a supported data format in a well-performing, yet flexible and extensible way.
% =======================================================================================================
\subsection{Basic Concept for Reading}%
\label{sec:BasicConceptforReading}%

In section \SectionLink{sec:RepresentingaDataFormat}, we discussed the approach that a data format is described in terms of a \emph{data format specification}, which basically is a description of how data is represented as a chunk of bytes in the data format. It breaks up such a chunk into different types of so-called data blocks that form a well-defined hierarchy (see \SectionLink{sec:TheContainerMetamodel}). Given such a specification and data block hierarchy, we define the following design decisions:

%%%% DD --> %%%%
\DD{dd:601}
{% Title
Data block instance classes defined in \COMPdataPartManagement{}
}
{% Short description
The instances for the different data block types are represented as Java classes in the \COMPdataPartManagement{} component which use the same names as in figure \ref{fig:II_GeneralModel}.
}
{% Rationale
As chunks of bytes are structured according to the metamodel and the data format specification describes them as such, it is quite clear the have to be instance of corresponding classes during parsing. These instances are also returned to the user and provide a clear, better-to-understand view on the data. Btw., having a metamodel but not transforming it into a class model seems to be quite nonsense.
}
{% Disadvantages
No disadvantages known.
}
%%%% <-- DD %%%%

%%%% DD --> %%%%
\DD{dd:602}
{% Title
Specification-driven Read Approach
}
{% Short description
Reading is heavily based on the description of the data format in its data format specification. The descriptions of lengths and types are used as basic assumption for parsing, especially magic keys for first identification.
}
{% Rationale
It is quite clear that otherwise, the data format specification would be quite useless and we would implement some data-format-specific and thus non-generic code again.
}
{% Disadvantages
Is it flexible enough? If we stick to a stiff specification, how to ensure that it is flexible enough for different data formats?
}
%%%% <-- DD %%%%

%%%% DD --> %%%%
\DD{dd:603}
{% Title
Reading and writing with \COMPmedia{}
}
{% Short description
Reading and writing is done using \COMPmedia{}, including use of its caching features.
}
{% Rationale
Again this is very clear.
}
{% Disadvantages
No disadvantages known.
}
%%%% <-- DD %%%%

Given these basic design decisions, we are ready to sketch a rough draft of how reading basically works. We first summarize the very fundamentals that we know this far:
\begin{itemize}
\item Data is represented as linear chunks of bytes whose structure is defined by the data format specification
\item The top-level data blocks for reading are containers
\item It is not clear which data format we have in front of us
\item A data block might get large, as we use long as data type (see \DesLink{dd:417})
\end{itemize}

Given the first two observations, we define the following:

%%%% DD --> %%%%
\DD{dd:604}
{% Title
Iterator approach for reading
}
{% Short description
  \LibName{} provides an iterator pattern for iterating top-level containers.
}
{% Rationale
As the basic structure of a chunk of data format bytes consists of linear disjoint containers and we need to do forward-reading, an iterator is a quite natural choice. Lists or other collection data types would be insufficient, because they would suggest that there is a finite number of containers. Iterators are a good fit for streaming, allowing virtually ``unlimited'' streams of containers.
}
{% Disadvantages
No disadvantages known.
}
%%%% <-- DD %%%%

Now we can define the basis approach of forward reading, i.e. what is done if we read a top-level container? Of course, first it is not clear which data format we have - identification is necessary, already introduced in \SectionLink{sec:MagicKeys}. Second, it might happen anytime that we hit end of medium, either expectedly or unexpectedly. The basic flow is defined in the following design decision and shown in figure \ref{fig:III_ForwardReading}.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=1.0\linewidth]{figures/III_ForwardReading.pdf}
  \caption{Steps for reading data forward}
  \label{fig:III_ForwardReading}
\end{figure}

%%%% DD --> %%%%
\DD{dd:605}
{% Title
Steps of forward reading of top-level containers
}
{% Short description
  Forward reading of a single top-level container works as follows - whenever \texttt{next()} on the iterator is called to read the first or next container:
  \begin{enumerate}
  \item First, we do data format identification, i.e. finding out whether the data chunk ahead belongs to a data format the library supports - see \SectionLink{sec:MagicKeys} for details. If we cannot identify the data format, the process stops here with an exception.
  \item After identifying the data format, we need to identify which concrete container type and id we have, i.e. determining its structure and actual id at runtime; this is especially necessary for data formats supporting generic containers which is most of the container data formats out there, or multiple different container types. 
  \item Then, as we forward-read, the headers of the container are parsed, which means understanding their content. This includes determining if there are other headers or footers, and also the length of the container's payload.
  \item After heaving determined the payload length, the payload object itself is created. 
  \item Then, if there are any footers, they are at least created and ready to be parsed.
  \item Finally, the container is created which consists of the previously created headers, footers and payload
  \end{enumerate}
  It might happen that during this process, an expected or unexpected end of medium occurs. Expected it can only be during data format identification, in all other cases there seems to be a corruption in the data or parallel changes, as the parsing metadata indicates a size which is not matching reality.

  During steps 2, 3 and 5, fields need to be parsed which is a special discipline discussed later.
}
{% Rationale
The process is aligned at the structure of a container and typical for forward reading.
}
{% Disadvantages
No disadvantages known.
}
%%%% <-- DD %%%%

Btw:
%%%% DD --> %%%%
\DD{dd:605a}
{% Title
Steps of forward reading for nested containers
}
{% Short description
These are nearly identical except that data format identification is unnecessary.
}
{% Rationale
Nested containers are other than the fact that the data format is already known in no way different to top-level containers.
}
{% Disadvantages
No disadvantages known
}
%%%% <-- DD %%%%

% =======================================================================================================
\subsection{Buffering, Caching and Lazy Reading}%
\label{sec:ReadingSteps}%

\COMPdataPartManagement{} uses the features of \COMPmedia{} for caching as described in \SectionLink{sec:PerfMedia}. These features are also used for buffering, i.e.
%%%% DD --> %%%%
\DD{dd:606}
{% Title
General buffering using maximum read-write block size is done - for each overlap buffer the next block
}
{% Short description
  Most of the read operations in \COMPdataPartManagement{} do a buffering before accessing data. They always buffer at most maximum read-write block size of bytes. If a buffering call detects that the start offset of buffering plus maximum read-write block size exceeds current buffered data, it buffers the next block starting behind the currently buffered data with at most maximum read-write block size of bytes.

  This decision strongly bases on the critical design decision \DesLink{dd:438a} for a sensible handling of objects (e.g. fields) overlapping two consecutive byte blocks read.
}
{% Rationale
Reading byte by byte or field by field would be nonsense from performance point of view. The maximum read-write block size is a natural fit for buffering. The approach of buffering the next block when an overlap is detected is sensible as it leads to less cache fragmentation. The previously cached consecutive block will not fall out of the cache thanks to design decision \DesLink{dd:438a}.
}
{% Disadvantages
No disadvantages known.
}
%%%% <-- DD %%%%

Now the question arises: Where to buffer exactly to ensure that there is no ``buffer gap''?

%%%% DD --> %%%%
\DD{dd:606a}
{% Title
Buffering at start of container and field reading
}
{% Short description
  It turned out to be fully sufficient if we do the following:
  \begin{itemize}
  \item Buffer at start of container identification/reading: Whenever it is checked if a container with a given type and id during container identification phase is done, we first buffer read-write block size of bytes. As this size has a lower bound (see \DesLink{dd:438a}) with a sensible default, this way all usual headers are already buffered, leading to speeding up data format identification for top-level containers.
  \item Buffer at start of field reading: Whenever a single field of a header, footer or payload is read, buffering is done. Due to \DesLink{dd:606}, any overlapping bufferings will actually read the next block in addition, ensuring that always as sensible amount of future bytes is pre-buffered. This case also covers field based payload.
  \end{itemize}
}
{% Rationale
Thus, even if we have unusually long headers, footers or field-based payload, there is never a buffering gap.
}
{% Disadvantages
No disadvantages known
}
%%%% <-- DD %%%%

A very basic mechanism of container data formats is that they allow seeking and skipping based on containers, that means the decoders might just need to check a container header to find out if they are interested in the data it contains, and if not simply go on reading with the follow-up container. For this, most if not all data formats specify the size of the container in its header or footer. To fully support this, we define some kind of lazyness as follows:

%%%% DD --> %%%%
\DD{dd:607}
{% Title
Lazy payload
}
{% Short description
Any field-based or container-based payload is lazy by default, i.e. its bytes are first read only when explicitly requested by the user. A sole exception is in place for stream-based media (see upcoming design decision). 
}
{% Rationale
This way, we do not read unnecessary data the user might never look at, especially allowing to skip over large containers without risking out of memory and long read times, just perfectly meeting requirement \SectionLink{sec:REQ009LesenSchreibenGrosse}.
}
{% Disadvantages
No disadvantages known
}
%%%% <-- DD %%%%

As streams are non-random-access media, the question arises what we do with the bytes of a stream. It is no good idea to skip the payload byte here - what if the user wants to closely look at them? There is no other choice than to still cache them. The same is true for rather ``exotic'' formats such as Lyrics3v2 where the payload size is not contained in a header, but only in the footer.

%%%% DD --> %%%%
\DD{dd:608}
{% Title
Fully cache payload for stream-based media and formats without size field
}
{% Short description
  For stream-based media, the whole payload data must be fully read to at least have a chance that the user can look at it later. It might happen that the maximum cache size is exceeded in which case the first bytes of the payload might fall out of the cache.

For formats not having a static payload size and not having a size indicator in its header, the payload must be fully read in entirety to know its size. This also leads to potentially filling up the cache. 
}
{% Rationale
  There is no other way for streams than grabbing the bytes when looking at them - streams are just one time read.

  For the special case of formats not having a payload size in their header, see \DesLink{dd:529d}.
}
{% Disadvantages
No disadvantages known
}
%%%% <-- DD %%%%

% =======================================================================================================
\subsection{Container Context Information}%
\label{sec:ContainerContextInformation}%

In section \SectionLink{sec:FieldFunctions}, we already mentioned the \emph{parsing metadata} that is vital for understanding and reading a container. This information is tagged as such by each extension in the form of field functions that link fields to target blocks they refer to. This information is of course relevant for generically parsing a container at runtime. As soon as you encounter e.g. a field that contains the size of the container's payload during parsing, you have to store this size somewhere to be accessible later when actually parsing the payload.

Here, we summarize all the parsing metadata known at one point in time during container parsing as the \emph{container context information}. However, the container context information shall not only be a simple map storing the already known field functions, but it is intended to be more:

%%%% DD --> %%%%
\DD{dd:620}
{% Title
\texttt{ContainerContext} provides sizes, counts, byte orders and character encoding of all data blocks in a container; it is available in each data block; one instance per container
}
{% Short description
A helper class \texttt{ContainerContext} provides sizes, counts, byte orders and character encoding of \emph{all} data blocks in the current container. That means it not only stores the field functions refering to some of them, but also knows about static sizes, static occurrences, default byte orders and character encodings etc. It is an attribute of each data block such that it can be accessed from most places during as well as after parsing. The details of size determination are described in \SectionLink{sec:SizeDetermination}, the details of count determination in \SectionLink{sec:CountDetermination}.
}
{% Rationale
Having just one place that not only stores field function values but also knows in a more abstract way how to determine sizes, counts, byte orders and character encodings allows for better separation of concerns. This also allows to pass around corresponding objects to everywhere you need this information in contrast to having this logic only in a reader class that is not availabe anywhere else after parsing.
}
{% Disadvantages
No disadvantages known
}
%%%% <-- DD %%%%

One question that comes up here: How about parsing metadata available in the top-level container, but needed in a child container? We define:

%%%% DD --> %%%%
\DD{dd:621}
{% Title
\texttt{ContainerContext} contains a reference to the direct parent container's context and searches the parent for metadata if none is found in itself
}
{% Short description
\texttt{ContainerContext} contains a reference to the direct parent container's context (if any). If it cannot come up with a size, count, byte order or character encoding itself, it asks the parent for such metadata.
}
{% Rationale
This way, parsing metadata only defined in the top-level container headers can be transported to children deeper in the hierarchy.
}
{% Disadvantages
No disadvantages known
}
%%%% <-- DD %%%%

% -------------------------------------------------------------------------------------------------------
\subsubsection{Size Determination}%
\label{sec:SizeDetermination}%

How to determine the sizes of data blocks before parsing? It is clearly necessary to know the size of a container in advance to be able to parse or skip it. Unfortunately, all data formats seem to follow a distinct strategy of how to specify sizes as indicated around design decision \DesLink{dd:540}.

The cases we have to handle are summarized as follows:
%%%% DD --> %%%%
\DD{dd:640}
{% Title
Cases for size determination
}
{% Short description
  The easiest \textbf{Case 1:} Static size indicated in the specification. If a data block's specification indicates a static size, this size is taken during parsing.

  Otherwise, the data block has a dynamic size which breaks down to the following cases:
  \begin{enumerate}
  \item[\textbf{Case 2:}] The data block's size is directly indicated by another field that must be parsed before (usually in header or footer) - Note that this size might include the size of other blocks in addition, see \DesLink{dd:542} and the discussion around for more details. Here, ``directly'' means that the field is a numeric value greater or equal to the size, and we do not need to calculate something.
  \item[\textbf{Case 3:}] The data block's size is indirectly indicated by other fields that must be parsed before (usually in header or footer). ``Indirectly'' means that the field is not just single or summed size contained in a field, but there is a more complex calculation of combining multiple field values to calculate the size. This is e.g. the case for the size of the MP3 container payload or the Ogg page payload and packet part sizes.
  \item[\textbf{Case 4:}] The data block is payload consisting of fields or containers, and there is no size indicator at all. However, it is clear how to determine the sizes of all children of the payload block, and thus the size of it is the sum of the sizes of all its children. This is e.g. the case for Lyrics3v2 tag payload.  
  \item[\textbf{Case 5:}] The data block is a terminated field, i.e. its size must be determined by finding its termination. The handling of this case is defined in \SectionLink{sec:TerminatedFields}.
  \item[\textbf{Case 6:}] None of the other cases applies, but at least the overall remaining size of the parent data block is known. Take the remaining size of the parent data block as size of the data block.
  \end{enumerate}

  \LibName{} supports exactly those cases. Despite case 5, they are implemented in the \texttt{ContainerContext}, see \SectionLink{sec:ContainerContextInformation}.
}
{% Rationale
The cases come from analyzing different data formats \LibName{} is required to support, so it is sufficient to consider only those (if any others exist). Why is case 5 not implemented in \texttt{ContainerContext}? The reason is that finding termination bytes requires parsing itself, so it must be implemented in the reader.
}
{% Disadvantages
No disadvantages known
}
%%%% <-- DD %%%%

Case 1 is already clear how to deal with, case 5 is treated later in \SectionLink{sec:TerminatedFields}. Thus we are left to deal with cases 2 to 4. First of all, we consider case 3:

%%%% DD --> %%%%
\DD{dd:641}
{% Title
Complex size calculation from fields done in custom implementation via \texttt{SizeProvider}
}
{% Short description
Data format extensions are allowed to register a custom \texttt{SizeProvider} implementation which calculates the size of a data block as required by the format. This is used for Ogg, ID3v2.3 extended header size and MP3.
}
{% Rationale
More flexibility also for yet other future formats.
}
{% Disadvantages
No disadvantages known
}
%%%% <-- DD %%%%

Case 4 is handled as follows:
%%%% DD --> %%%%
\DD{dd:642}
{% Title
Determine payload size by reading all children
}
{% Short description
In case 4 of \DesLink{dd:640}, the size of the payload is calculated by reading all children, may it be containers or fields, and summing up their sizes. If at least one of their sizes could not be determined, this is a runtime exception.
}
{% Rationale
Straightforward and not too complex in terms of ``lazyness''. If the data formats design is fucked up, the implementation cannot easily work around it.
}
{% Disadvantages
No disadvantages known
}
%%%% <-- DD %%%%

Now we have case 2 left. This one is adressed with using field functions - potentially referring to multiple consecutive blocks as indicated by \DesLink{dd:540}. We clarify how it works by summing up the whole progress of size determination in total:

%%%% DD --> %%%%
\DD{dd:644}
{% Title
Size determination process
}
{% Short description
  Whenever the size of a single data block needs to be determined, the following steps are performed in the given order of precedence:
\begin{enumerate}
\item (Outside of \texttt{ContainerContext}) If the data block is a terminated field without fixed size, search for the termination character (case 5)
\item Otherwise if the current data format has a registered \texttt{SizeProvider} (see \DesLink{dd:641}), first ask this instance if it can provide a defined size (includes also case 3)
\item Otherwise if the data block has a static size according to its specification, take this size (case 1)
\item Otherwise if the current \texttt{ContainerContext} has a \texttt{SizeOf} field function, take the size indicated by the \texttt{SizeOf} field (part 1 of case 2)
\item Otherwise if the current \texttt{ContainerContext} has a \texttt{SummedSizeOf} field function for multiple data blocks and the sizes of all other blocks are known, take the \texttt{SummedSizeOf} field's value minus the summed size of the other data blocks (part 2 of case 2) - How to avoid an infinite loop here? This has been treated in design decision \DesLink{dd:541} and is also discussed with an example after this design decision.
\item Otherwise if the data block is a concrete data block that is derived from a generic data block, it is checked if there is a known size for the generic id  
\item Otherwise if the parent \texttt{ContainerContext} has a \texttt{SizeOf} field for it, take the size of the parent \texttt{ContainerContext}
\item Otherwise if the data block is a field or payload and the remaining parent size is known, take the remaining direct parent size as its size (case 6)
\item (Outside of \texttt{ContainerContext}) Otherwise if the data block is a payload block, read all its children and sum up their sizes (case 4)
\end{enumerate}
}
{% Rationale
The order of steps clearly focusses on least complexity first and tries to avoid unnecessary lookups by first considering fixed sizes and custom size providers.
}
{% Disadvantages
No disadvantages known
}
%%%% <-- DD %%%%

Some subtle point in determining sizes of single data blocks whose \texttt{SummedSizeOf} field is the sum of multiple consecutive data blocks is: If we want to determine the size of a single one, we have to determine the size of all the others, sum this size and subtract it from the value stored in the \texttt{SummedSizeOf} field. If for some stupid reason there are multiple data blocks included with a dynamic size, this could end up in an infinite loop such as for ID3v2.3: Size of payload needs to be determined, thus determine the size of the extended header first. However, the size of the extended header is also dynamic. The implementation would see that it would need the size of the payload to calculate the size of the extended header and thus call the first function again... To avoid this crap, we already stated the validity criteria in \DesLink{dd:541}.

% -------------------------------------------------------------------------------------------------------
\subsubsection{Count Determination}%
\label{sec:CountDetermination}%

Here we discuss how the determination of the number of occurrences of a data block (i.e. count determination) works. First of all, we can state that the number of payload containers is always 1, so no need to consider this case. For all other types of data blocks, there can be 0 to multiple occurrences.

The process to handle is summarized as follows:

%%%% DD --> %%%%
\DD{dd:645}
{% Title
Count determination process
}
{% Short description
  Whenever the count of a single data block needs to be determined, the following steps are performed in the given order of precedence:
\begin{enumerate}
\item If the current data format has a registered \texttt{CountProvider}, first ask this instance if it can provide a defined size
\item Otherwise if the data block has a static number of occurrences according to its specification, take this count
\item Otherwise if the data block is optional, i.e. either is present once or not at all, and if the current \texttt{ContainerContext} has a \texttt{PresenceOf} field function, take 1 as count if its presence is indicated by the flags, 0 otherwise
\item Otherwise if the current \texttt{ContainerContext} has a \texttt{CountOf} field function, take the count indicated by the \texttt{CountOf} field
\item Otherwise if the parent \texttt{ContainerContext} has a \texttt{CountOf} field for it, take the size of the parent \texttt{ContainerContext}
\end{enumerate}
}
{% Rationale
The order of steps clearly focusses on least complexity first and tries to avoid unnecessary lookups by first considering fixed counts and custom count providers.
}
{% Disadvantages
No disadvantages known
}
%%%% <-- DD %%%%

% =======================================================================================================
\subsection{Field Parsing}%
\label{sec:FieldParsing}%

Field parsing is the most complex part of reading data format bytes. It requires answers to following questions:
\begin{itemize}
\item How to determine the size of the field? - This has been answered already in \SectionLink{sec:SizeDetermination}, except the case of terminated fields which we treat here
\item How to determine the number of occurrences of a field, especially the case of optional fields? - This has been answered already in \SectionLink{sec:CountDetermination}
\item Which byte order (for numeric fields) and character encoding (for string or enumerated fields) needs to be used when? - This has been answered already in \SectionLink{sec:ContainerContextInformation}
\item How to convert the binary field value to an interpreted one?
\item How to use caching during field parsing?
\end{itemize}

% -------------------------------------------------------------------------------------------------------
\subsubsection{Terminated Fields}%
\label{sec:TerminatedFields}%

Terminated fields are those fields with a dynamic length that is not given by the value of another field, but these fields are delimited by a sequence of termination bytes or characters. 

Terminated fields are worst-case for parsing: First of all you never know when it ends, and second you have to deal with strings which brings different encodings into play.

Let's deal with the first problem, the unknown length. Of course, in the ``usual'' case a terminated field is still a field and thus relatively small. We will only rarely encounter terminated fields longer than several KB. In order to be fully generic, one needs to deal with the worst case of fields having up to \texttt{Long.MAX\_VALUE-1} as size. Having said this, also block-wise reading is required as we do not want to risk out-of-memory conditions by soaking too much bytes into memory. One could come up with the idea of using some upper bounds for reading data, i.e.:
\begin{itemize}
\item \textbf{Case 1:} The current total medium size as well as the exact size of a parent of the field is known - E.g. for ID3v23 text frames, the frame size and thus payload size is known, but there is a terminated text payload field; for file and byte array media, the medium length is also known.
\item \textbf{Case 2:} The current total medium size is unknown, yet the exact size of a parent of the field is known - Take an ID3v23 text frame on an input stream medium as an example.
\item \textbf{Case 3:} The current total medium size is known, but the exact size of a parent of the field is unknown - Take an APEv2 item header on a file or byte array medium as an example. Here, the item key is terminated, and the size of the parent header is unknown as it is only determined by the sum of the field sizes and not - in addition - by any size indicators.
\item \textbf{Case 4:} Neither current total medium size nor the exact size of a parent of the field is known - So APEv2 item header on an input stream medium is an example.
\end{itemize}

%%%% DD --> %%%%
\DD{dd:650a}
{% Title
Block-wise reading for finding field termination
}
{% Short description
Block-wise reading is necessary in order to find field termination. According to \DesLink{dd:438}, we do not read more than the configured maximum read-write block size in bytes from the medium at once. Actually, we \emph{always} read the configured maximum read-write block size and do not take into consideration any additionally known upper bounds such as the remaining parent byte count or remaining medium length (if available). If an end of medium is encountered during reading a block of data, we scan for the termination within the actually read bytes. Otherwise the next block is read for scanning for termination bytes and so on. However, we can at least take into account any available bounds for finding out if we need to search further or we can skip the process: If the already read byte count is bigger than the remaining parent byte count, we should skip as a data corruption might be present.
}
{% Rationale
Block-wise reading avoids out-of-memory conditions and is a compromise between reading too much and reading too few bytes at once. Of course this compromise comes with the cost of added complexity. We mitigate this complexity by not coming up with further ``optimizations'' such as only reading up to a known upper bound like the medium end or parent end. In the end, both might not be known, especially in case 4, thus anyway we have to do a ``blind'' reading in this case.
}
{% Disadvantages
Added complexity, but in conformance with \DesLink{dd:438}. The complexity is adressed as specified in the rationale.
}
%%%% <-- DD %%%%

Regarding strings and their encoding: First of all, we remember that only string and string list fields can be terminated and thus it is fine to talk about \emph{termination characters} instead of termination bytes (see \DesLink{dd:531} and \DesLink{dd:532}). How should we parse terminated fields to detect the termination character? We could do it either:
\begin{enumerate}
\item by converting the character to a byte sequence and then searching the termination
\item by converting the bytes to a string and then trying to find the termination character or
\end{enumerate}

Both approaches have their serious drawbacks. \textbf{Approach (1)} requires us to know where a character starts and ends such that we can correctly match termination bytes only at character borders. For instance UTF-16 encodes all characters always as two bytes. It would be wrong to try to detect termination bytes at odd offsets. Instead we have to ensure to search for them only at even offsets.

\textbf{Approach (2)} suffers from a similar problem that there is no nice way to handle block-wise split strings: If we read block-wise, it might be that a single character spanning multiple bytes is just torn apart at a block border. E.g. UTF-8 is a variable length encoding where - depending on the first byte - the total number of bytes of a character is given. If a converter from byte to string encounters a torn apart character at the end of the byte sequence, it might fail with some kind of decoding runtime error, so we have to handle this separately. However, Java's \texttt{CharsetDecoder} seems just about right for this task. ID3v2.4 supports UTF-16LE, UTF-16BE, ISO-8859-1 as well as UTF-8 so we have to handle such cases.

We define:
%%%% DD --> %%%%
\DD{dd:650}
{% Title
Character-based termination detection
}
{% Short description
We use character-based termination detection (approach 2) for finding termination characters and do not use termination-byte-based detection (approach 1)
}
{% Rationale
This is possible due to the fact that only string and string list fields can be terminated (see \DesLink{dd:531} and \DesLink{dd:532}). Approach 1 seems even harder as we need to re-implement parsing of strange encodings while approach 2 at least provides an out-of-the box solution in the form of  \texttt{CharsetDecoder}.
}
{% Disadvantages
No disadvantages known
}
%%%% <-- DD %%%%

No we can summarize the process of finding termination of a field:
%%%% DD --> %%%%
\DD{dd:651}
{% Title
Determining the end of a terminated field
}
{% Short description
  As input for the algorithm, we get the current medium offset, the character encoding of the bytes as well as the termination character to find and the maximum read-write block size. The steps are as follows:
  \begin{enumerate}
  \item Read $N:=$ read-write block size bytes from the medium at the current offset
  \item If end of medium occurs, take the actually read bytes and mark the algorithm for termination as no further input is available
  \item Search the input buffer for the termination character by actually decoding bytes according to the character encoding using \texttt{CharsetDecoder}
  \item If there should be a character overlapping a block border, use the facilities of \texttt{CharsetDecoder} to overcome the situation
  \item If a termination character is found, mark the algorithm for termination
  \item Otherwise:
    \begin{enumerate}
    \item If the remaining parent byte count is known and the already read byte count is bigger than this value, mark the algorithm for termination 
    \item Otherwise increment the current medium offset by the maximum read-write block size and goto step 1
    \end{enumerate}
  \end{enumerate}
}
{% Rationale
This process is efficient in terms of avoiding unknown steps and using only sparse memory. It is as simple as it can get without risking incorrectness.
}
{% Disadvantages
No disadvantages known
}
%%%% <-- DD %%%%

% -------------------------------------------------------------------------------------------------------
\subsubsection{A Need for Lazy Fields?}%
\label{sec:LazyFields}%

In this section, we want to investigate if we need a kind of lazy field (similar to lazy payload as described in \DesLink{dd:607}). The idea: What if a field is large and we know this in advance? Should we read all field bytes into memory or rather not?

First of all, we shall check if there are cases that clearly come to mind here:
\begin{itemize}
\item Ogg contains segments within its payload that only get a max size of 255, so no need here for lazy fields; also the ogg header is quite harmless
\item APEv2, Lyrics3v2, ID3v1 and ID3v2 only define small fields
\item MP3 payload fields might not be longer than 998 bytes, so no lazyness required here
\item RIFF, QuickTime and Matroska might have arbitrary large payload, and thus arbitrary large fields to read
\end{itemize}

It looks like that it is unavoidable that large fields might occur. However, there is still an easy way to circumvent this and define a maximum size, and no field lazyness:

%%%% DD --> %%%%
\DD{dd:652}
{% Title
Fields must only have a maximum size of \texttt{Integer.MAX\_VALUE}, no lazy field mechanism 
}
{% Short description
There is no special case for large fields in \LibName{}. Instead, we simply define that the maximum length of a field must not exceed \texttt{Integer.MAX\_VALUE}. This is already checked during specification validation. If a need arises to support unstructured payload longer than this, the specification can simply define a field with maximum size \texttt{Integer.MAX\_VALUE} and more than one occurrence.
}
{% Rationale
Avoids unnecessary complexity due to yet another special case.
}
{% Disadvantages
In theory, if fields actually reach \texttt{Integer.MAX\_VALUE} and the user actually reads the field, an OOM might occur.
}
%%%% <-- DD %%%%

% =======================================================================================================
\subsection{Backward Reading}%
\label{sec:BackwardReading}%

The feature of reading backward is just a convenience and performance improvement functionality. It has a lot of differences compared to forward reading, i.e. it is not only just ``reversed forward reading''. The reason is that APIs do not know some kind of ``backward read mode'', i.e. file I/O and array APIs only allow to read forward starting at offset $x$. The problem with this is that you do not know where $x$ is during backward reading, because e.g. $x$ is the starting point of a dynamic length data block, but you are currently positioned at its end and have no idea where it starts.

Nevertheless, some data formats support backward reading by providing a so-called footer. The reason for this feature is that multimedia files are easier to modify at their end than at the beginning: Wen inserting a tag of length $n$ at the beginning of the file, you actually have to read all bytes of the file, write them to offsets $x+n$, and then write the tag bytes at offset 0. This requires a lot of I/O and can be expensive, especially for large files. Appending tags at the end of file avoids this. To ensure that parsers can identify such end-of-file tags, footers have to be defined and present. From the supported formats, Lyrics3v2, ID3v2.4 and APEv2 support footers. ID3v1 can be read backwards, too, as it is of static length.

However, first of all, we have to note down:
%%%% DD --> %%%%
\DD{dd:653}
{% Title
Streaming media cannot be read backwards
}
{% Short description
\LibName{} cannot read stream-based media backwards and reacts with a runtime exception if a user tries to do that
}
{% Rationale
Random access media allow starting reading from any point, streams by design do only start at the beginning.
}
{% Disadvantages
No disadvantages known
}
%%%% <-- DD %%%%

If we do backward reading, the first question that might come up is: How do we actually do it? Quite similar to forward reading but with some important differences:
%%%% DD --> %%%%
\DD{dd:654}
{% Title
Backward reading uses footer magic keys
}
{% Short description
When backward reading, \COMPdataPartManagement{} tries to find a footer magic key at a fixed position in front of the current offset. If it can find it there, this is a signal that a container of the given data format is present. The implementation tries to extract a static payload size from the footer(s). Then it reads the payload using this static size by going that much bytes backward. Finally, it reads any header(s) in front. If start of medium is encountered during this, this is quite similar to an end of medium unexpected error.
}
{% Rationale
This allows to reuse some parts of forward-reading and thus avoids to invent something totally different.
}
{% Disadvantages
No disadvantages known
}
%%%% <-- DD %%%%

\DesLink{dd:654} did mention a basic approach and also footer(s) and header(s). But how to deal with dynamic length headers or footers or unknown length payload? The problem here is that you only know where a footer, payload or header ends, but not where it actually starts if it is dynamic or unknown length. For now, we state:

%%%% DD --> %%%%
\DD{dd:655}
{% Title
Only static length headers, payload and footers are supported for backward-reading
}
{% Short description
As of the current release, \LibName{} cannot deal with dynamic length headers or footers during backward reading. It throws a runtime exception if the presence of such a dynamic-length header is detected. The same is done if the lenght of payload could not be determined. However, \LibName{} fully supports multiple static-length headers or footers as well as optional ones, with quite the same mechanisms as for forward-reading. 
}
{% Rationale
This is e.g. the case for ID3v2.4: If there is an extended header (which has dynamic length) there is no way to find out where it starts during backward reading except you search for the actual main header starting point. This would somehow not exactly fit with the forward-read approach. There is an option to implement it later or for the extensions to overwrite the default implementation in a more flexible way.
}
{% Disadvantages
No disadvantages known
}
%%%% <-- DD %%%%

One observation that is important to grasp comes when you think about parsing the content of a container that has already been backward-read:

%%%% DD --> %%%%
\DD{dd:656}
{% Title
Payload content of a backward-read container is forward-read
}
{% Short description
As soon as we have fully read a container backwards, we of cause know where its payload starts and ends. Thus, we can of course read the payload \emph{forward} instead of backward. The same is of course true for headers and footers during backward-reading: As we know where they start, we wouldn't want to read their fields somehow backwards. Instead we simply reuse the existing code that forward-reads fields.
}
{% Rationale
We can reuse a lot of code for forward-reading instead of coming up with new code. Decreases complexity.
}
{% Disadvantages
No disadvantages known
}
%%%% <-- DD %%%%

Finally, we look at a special case of backward reading: ID3v1.

%%%% DD --> %%%%
\DD{dd:657}
{% Title
ID3v1 can be backward-read as it is static size, this is done in the extension
}
{% Short description
As ID3v1 has a static size of 128 bytes, we can always check if there is the header's magic key at -128 bytes from the current offset. If so, we've found the tag during backward reading. However, as ID3v1 does not have a footer but only a header, this special case has to be implemented in a custom implementation of the extension and not in the generic centralized code.
}
{% Rationale
ID3v1 frequently occurs at the end of a medium and thus it makes sense to support this.
}
{% Disadvantages
No disadvantages known
}
%%%% <-- DD %%%%

% =======================================================================================================
\subsection{Basic Concept for Writing}%
\label{sec:BasicConceptforWriting}%

Here, we care about the basics for writing. Writing data is an intgral part of the \COMPdataPartManagement{} component. The question is: How can a user manipulate data? What is already clear is that we cannot write to stream-based media and the attempt will lead to a runtime exception (see \SectionLink{sec:ZugriffEinesTERMmedium}).

What can be already noted is that we will of course use the features of \COMPmedia{}:
%%%% DD --> %%%%
\DD{dd:660}
{% Title
Using \COMPmedia{} with two-staged write protocol
}
{% Short description
\COMPdataPartManagement{} uses the \COMPmedia{} API to perform the writing. Thus, it also has to adhere to the two-stage write protocol as described in \SectionLink{sec:GrundSchreiben}. That means that first, changes are done as needed, and then the user needs to perform an actual flush of the changes in a second step. For read-only media, an exception is thrown only when trying to flush changes.
}
{% Rationale
Otherwise we needn't do the fuss in \COMPmedia{}. Two-stage writing is sensible as usually one change is not done isolated but leads to a series of other changes. The user can safely do the changes and then ``commit'' at a sensible time, leading to even a better performance.
}
{% Disadvantages
No disadvantages known
}
%%%% <-- DD %%%%

However, if we do it like that, one question immediately arises: What does the user ``see'' after having done changes but not yet having flushed them? For instance, the user may add an extended header to an ID3v2.3 tag or remove a frame from it. If the user then iterates the frames, will the removed frame still be returned or not? 

%%%% DD --> %%%%
\DD{dd:661}
{% Title
The user sees the dirty state of the containers when calling getter methods with just one exception (see next design decision)
}
{% Short description
Once the user has done changes on a \IMediumStore{} using the writing API of \COMPdataPartManagement{}, these changes will be visible and active in the reading API of \COMPdataPartManagement{}, even if they are not yet flushed. Removed blocks won't be returned anymore, inserted blocks will be returned, and changed field values will also be returned. The user would not see these changes using another \IMediumStore{} to access the medium. However, this is not possible due to the locking concept of \COMPmedia{}.
}
{% Rationale
  This seems to entirely contradict with \DesLink{dd:410c} that states: \COMPmedia{} read methods always return the currently persisted state. However, the written data is still available in terms of added data blocks.
  
It would be quite strange for the user if data blocks previously inserted would not be returned again when reiterating the blocks. This could cause confusion for the users and would e.g. not offer the possibility to undo changes if the user did not keep a reference to an insterted block.
}
{% Disadvantages
No disadvantages known
}
%%%% <-- DD %%%%

Here comes the sole exception:
%%%% DD --> %%%%
\DD{dd:661b}
{% Title
Retrieval of raw bytes always only returns currently flushed state
}
{% Short description
Retrieval of raw bytes always only returns currently flushed state. That means it will still return bytes for currently removed data blocks, but it won't return bytes for inserted or new data blocks.
}
{% Rationale
Read methods in \COMPmedia{} only return the medium state (see \DesLink{dd:410c}), because doing things otherwise would have made it all even more complex. Likewise, also here, instead of building castles in the sky just for the rare case that a user might find it convenient to get the bytes according to the not-yet-persisted dirty state, we won't waste much time. Javadocs could state this behaviour easily.
}
{% Disadvantes
No disadvantages known
}
%%%% <-- DD %%%%

In section \SectionLink{sec:FieldFunctions} we introduced the so-called field functions as a way to indicate dependencies between fields and other data blocks. However, just modeling this dependency without using it in anyway is of course nonsense. The main driver for introducing this concept is consistency: If you have a size field indicating the size of payload, and if that payload changes, we want the size field to change consistently without manual intervention of the user. Thus, we have to implement this feature during writing. This goes hand in hand with the two-stage write protocol decided in \DesLink{dd:660}.

%%%% DD --> %%%%
\DD{dd:662}
{% Title
Fields with functions are consistently updated during writing 
}
{% Short description
Whenever a user changes a datablock that is referenced by another field with a field function, the field's value must be updated according to the kind of change: \texttt{SizeOf}, \texttt{SummedSizeOf}, \texttt{CountOf} and \texttt{PresenceOf} are updated once a user writes to data blocks these field functions refer to. The fields themselves need to be read-only to ensure consistency. On the other hand, \texttt{CharacterEncodingOf} and \texttt{ByteOrderOf} fields can be written, leading to an inverse change to all data blocks they refer to: If you change the fields value, the byte order or character encodings of the target data blocks must change automatically, leading to a potential change to any numeric or string target fields. The two-stage write protocol of \DesLink{dd:660} minimizes the chance that there is an inconsistent state where the field value changes but not the referred data blocks, because the change is first getting only scheduled consistently, and later written in a ``single'' operation. 
}
{% Rationale
Consistency and user convenience
}
{% Disadvantages
Of course this makes the implementation more complex and in the first place requires the concept of field functions.
}
%%%% <-- DD %%%%

A second inference from using a two-stage write protocol is the possibility of a ``rollback'' - which MUST not be confused with ACID in any way:
%%%% DD --> %%%%
\DD{dd:663}
{% Title
Undo possibilities of unflushed changes are necessary
}
{% Short description
It needs to be necessary that the user can undo all changes he has done before a flush. However, it must be clearly communicated in the API and documentation that this whole thing is in no way ACID! Thus we do not actually call it ``rollback''. Flushing changes can go wrong in the middle, leaving part of the changes already written to the medium. Likewise, ``rollback'' could fail in between, but will at least not messing up the medium, as nothing has been written yet. See \DesLink{dd:410b} for further details.
}
{% Rationale
If having a two-stage write protocol, this desire comes to mind. If I can mark something for change, why can I not undo it? The second thing is field functions (\DesLink{dd:662}): If we change fields according to other changes, then being able to undo these connected changes also in a consistent way is necessary. Another argument is that we have some possibility to do that in the undo functionalities of \COMPmedia{}, see \DesLink{dd:420}.
}
{% Disadvantages
Implementing this of course requires some effort: You need to ensure that every change is properly undone.
}
%%%% <-- DD %%%%

Having talked about consistency, it comes to mind: Whenever a user changes something or creates something new, there are a myriad of possibilities why this change is utterly against the data formatt's specification. A field value could be invalid, the size of something could be too big or small, the type or structure of the thing to add could be tried to be added at the complete wrong place etc. How do we deal with such things?

%%%% DD --> %%%%
\DD{dd:662}
{% Title
Strict validation of specification conformance as early as possible
}
{% Short description
For every writing action, \COMPdataPartManagement{} strictly checks the validity of the things done against the data format specification. If the change violates properties of the data block changed or its parent, this is clearly and directly communicated with a runtime exception. The details are defined in later individual design decisions.
}
{% Rationale
Fail fast is a good principle, here even more. We want to avoid the user to write any crap to the medium that cannot be re-read, as it might be utter nonsense.
}
{% Disadvantages
No disadvantages known
}
%%%% <-- DD %%%%

Already now, it clearly emerges that the writing API will be fundamentally different from the reading API:
%%%% DD --> %%%%
\DD{dd:663}
{% Title
Writing methods are scattered in the API in contrast to reading
}
{% Short description
There is a fundamental asymmetry between reading and writing: While reading is done using an iterator approach and a core class that does the reading, writing is done by methods associated with the datablocks themselves. For extensibility this means that there is no single class the user can subclass to extend the writing functionality.
}
{% Rationale
Reading is done step by step at once in a linear process, while writing means to manipulate the individual objects that have previously been read or created.
}
{% Disadvantages
Writing is harder to extend, but currently there seems to be no need to do so
}
%%%% <-- DD %%%%

% =======================================================================================================
\subsection{The State of a Datablock}%
\label{sec:StatesofDatablocks}%

If \LibName{} would only support reading of datablocks, a datablock state would probably not be necessary. However, if datablocks are modified and as we have a two-stage write protocol, states become indeed a necessity, as actions depend on the current state of a datablock. We can think of the following states:
\begin{longtable}{|p{0.15\textwidth}|p{0.8\textwidth}|} \hline
  \textbf{State} & \textbf{Description} \\ \endhead \hline
  \texttt{NEW} & The datablock has been newly created or persistently removed from the medium. That means it is not attached to a parent datblock and does not have an offset. \\
  \hline
  \texttt{PERSISTED} & The datablock is persisted on a medium and thus has an offset. There are no modifications in the datablock that are not yet flushed. This is the state of a datablock after reading. \\
  \hline
  \texttt{INSERTED} & The datablock has been newly inserted into the medium and attached to a parent, but this change was not yet flushed. \\
  \hline
  \texttt{REMOVED} & The datablock has been removed from the medium and detached from a parent, but this change was not yet flushed. \\
  \hline
  \texttt{MODIFIED} & The datablock has been modified, but this change was not yet flushed. \\
  \hline
  \caption{Datablock states}
  \label{tab:Datablockstates}
\end{longtable}

%%%% DD --> %%%%
\DD{dd:670}
{% Title
A datablock in \LibName{} has one of the states listed in table \ref{tab:Datablockstates}.
}
{% Short description
A datablock in \LibName{} has one of the states listed in table \ref{tab:Datablockstates}.
}
{% Rationale
Depending on the state, some actions are legal or illegal, or need to be performed differently. Further on, the user must have transparency what has been done with a block so far, so the state is a good indication. Other states than the ones mentioned are not required. A separate \texttt{MODIFIED} is needed due to fields whose values can be changed.
}
{% Disadvantages
No disadvantages known
}
%%%% <-- DD %%%%

The questions remaining now are: How do states exactly change? And what about the hierarchical dependencies between datablocks? The latter question needs to be investigated first. What if a parent of a datablock is removed, does state of the child change correspondingly?

%%%% DD --> %%%%
\DD{dd:671}
{% Title
If a datablock changes state, all its descendant blocks change to the same state
}
{% Short description
Any change to a datablock leads to the same state change in all its children and further descendants.
}
{% Rationale
Obviously, all descendants are contained within the bounds of the datablock. So if it becomes \texttt{NEW}, \texttt{PERSISTED}, \texttt{INSERTED}, \texttt{REMOVED} or \texttt{MODIFIED}, all its descendants also do.
}
{% Disadvantages
No disadvantages known
}
%%%% <-- DD %%%%

What about a change in a child or other descendant, does it have any effect on its parent?
%%%% DD --> %%%%
\DD{dd:672}
{% Title
Modification, insertion or removal of a descendant leads to all its parents transitioning to \texttt{MODIFIED} state  
}
{% Short description
A datablock changes into the \texttt{MODIFIED} state once any of its descendants are modified, inserted or removed.
}
{% Rationale
A change in any child leads to an actual change of data contained in the datablock. It would be misleading if the API would still say the datablock is \texttt{PERSISTED}, no matter there are unflushed changes in a child. Furthermore, implementing things such as flushing or undo will probably be easier if you can tell which top-level data blocks were modified in some way instead of needing to search all children until the leaf nodes of the hierarchy to find out.
}
{% Disadvantages
No disadvantages known
}
%%%% <-- DD %%%%

Now we have all ingredients to come up with a suitable state transition diagram, without actual method calls yet, see figure \ref{fig:III_DBStates.pdf}.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=1.0\linewidth]{figures/III_DBStates.pdf}
  \caption{Datablock state transitions}
  \label{fig:III_DBStates.pdf}
\end{figure}

% =======================================================================================================
\subsection{Writing Actions}%
\label{sec:WritingActions}%

In this section, we clarify in detail which write actions there are and how they need to work.

% -------------------------------------------------------------------------------------------------------
\subsubsection{Creating New Data blocks}%
\label{sec:CreatingNewDatablocks}%

First of all, in order to insert new data blocks, the user must be able to create new data blocks. The natural place to go is a factory:
%%%% DD --> %%%%
\DD{dd:675}
{% Title
A factory to create new data blocks
}
{% Short description
A factory instance accessible by the user is required to create new instances of containers, fields, headers, footers or payload. The created data blocks have the state \texttt{NEW}. The user specifies the id of the data block to create. The factory ensures that the given id belongs to the expected data format and has the correct type. Further validations are only done during insertion only. Of course for the user being able to specify an id during creation, every id must be publicly available in the API of the data format extension.
}
{% Rationale
A factory is a class that can create a family of products. This is clearly the case for data blocks. Different implementations can be created in future, if necessary. There is no direct dependency from user code to concrete data block classes. Some validations are done here already according to \DesLink{dd:662}.
}
{% Disadvantages
No disadvantages known
}
%%%% <-- DD %%%%

Another question to be answered is about the lifecycle of the factory. Should it be stateless and singleton or is there any use in having multiple instances?
%%%% DD --> %%%%
\DD{dd:675b}
{% Title
One factory instance per medium
}
{% Short description
There is exactly one instance of the factory per medium and reader.
}
{% Rationale
The only reason for this is that thus the create methods may have less parameters, as we can pass reader, specification and \IMediumStore{} to the factory via constructor instead of passing them every time.
}
{% Disadvantes
No disadvantages known
}
%%%% <-- DD %%%%

% -------------------------------------------------------------------------------------------------------
\subsubsection{Inserting Datablocks}%
\label{sec:InsertingDatablocks}%

\texttt{NEW} datablocks are inserted in their parent or at the top-level of the medium, respectively:
%%%% DD --> %%%%
\DD{dd:676}
{% Title
Insert methods for headers, footers and fields in their parent data block
}
{% Short description
To insert new headers, footers or fields, there are corresponding insert methods that do the job in the corresponding datablock class. They take an index that must be valid.
}
{% Rationale
You anyway need to have the parent instance, thus it makes sense to perform insertion in the parent itself, as the children are also anyway part of the parent.
}
{% Disadvantages
No disadvantages known
}
%%%% <-- DD %%%%

%%%% DD --> %%%%
\DD{dd:677}
{% Title
Insert methods for containers in container iterators
}
{% Short description
For both top-level as well as payload container iterators, there is an insert method in the iterator inserting a new container before the next container. That means to insert at a specific position, users first need to iterate to it. Containers can be inserted/appended at the end of the medium as soon as hasNext returns false. 
}
{% Rationale
This is quite similar to the insertion of headers, footers and fields.
}
{% Disadvantages
No disadvantages known
}
%%%% <-- DD %%%%

Now what about payload datablocks? For these insert methods wouldn't make much sense. Thus we define:
%%%% DD --> %%%%
\DD{dd:678}
{% Title
Simple setter method for payload datablocks
}
{% Short description
Payload children can be set with a setter method
}
{% Rationale
There is always just exactly one payload in a container
}
{% Disadvantages
No disadvantages known
}
%%%% <-- DD %%%%

One thing for parent datablocks containing lists of children: Can a user modify the returned lists?
%%%% DD --> %%%%
\DD{dd:679}
{% Title
Lists of headers, footers and fields are unmodifiable for the user, but are internally updated by the insert methods
}
{% Short description
A user can retrieve the child headers, footers or fields as a list. However, it is prevented that the user adds new children or otherwise modifies theses lists. Insert methods take care to also insert the child into the internal lists, if necessary.
}
{% Rationale
This way, there is no way in messing up the internal lists, and it is quite clear that the insert methods are the intended way to go.
}
{% Disadvantages
No disadvantages known
}
%%%% <-- DD %%%%

In \DesLink{dd:662} it was said that specification validity should be ensured as early as possible, for insertion that means:
%%%% DD --> %%%%
\DD{dd:680}
{% Title
Specification validations during insertion
}
{% Short description
The following checks against the specification are performed when inserting a new child, and if these checks fail, a runtime exception is thrown:
\begin{itemize}
\item Is the datablock id of the inserted block indeed a child of this datablock?
\item Can it be inserted at the given position, i.e. is it specified to be located there?
\item Is there already a datablock of the same type, and maximum occurrences are already reached?
\item Is the parent read-only?
\item Is the maximum parent size exceeded with this insertion (e.g. is the parent fixed size)?
\item Is the inserted child of the expected datablock type, e.g. indeed a header, footer or field as indicated by the insert method called?
\item Is the inserted child complete in a sense: Does it have all mandatory children?
\end{itemize}
}
{% Rationale
In accordance to \DesLink{dd:662}
}
{% Disadvantages
No disadvantages known
}
%%%% <-- DD %%%%

If all these checks are done, further validations regarding the parent and child state are necessary:
%%%% DD --> %%%%
\DD{dd:681}
{% Title
Required datablock states
}
{% Short description
The datablock to insert must have the state \texttt{NEW}. The parent datablock must have one of the states \texttt{NEW}, \texttt{PERSISTED}, \texttt{MODIFIED} or \texttt{INSERTED}, i.e. it must not be \texttt{REMOVED}. If one of these conditions is not met, a runtime exception is thrown.
}
{% Rationale
Allowing other states than \texttt{NEW} for the to-be-inserted block would lead to strange use cases such as ``remounting'' an already persisted datablocks which we want to avoid. It is thus clear to the user: If I want to insert a data block, I have to newly create it or at least remove and flush it before. The parent state must not be \texttt{REMOVED}, because what sense would it make to add new children to a parent block which is entirely removed anyway on next flush?
}
{% Disadvantages
No disadvantages known
}
%%%% <-- DD %%%%

The next question regarding states is: How do states change after the insert?
%%%% DD --> %%%%
\DD{dd:682}
{% Title
State changes caused by an insert
}
{% Short description
If the parent block is \texttt{NEW}, the inserted datablock remains \texttt{NEW}. Otherwise the inserted block and all its descendants are changed into state \texttt{INSERTED} while the parent becomes \texttt{MODIFIED}, if not yet in that state.
}
{% Rationale
The states thus are quite conceivable.
}
{% Disadvantages
No disadvantages known
}
%%%% <-- DD %%%%

As described in \DesLink{dd:662}, we need to adress fields with field functions during insertion. How this is done is described in the following design decision:
%%%% DD --> %%%%
\DD{dd:683}
{% Title
Changes in field functions fields during inserts
}
{% Short description
  If a datablock is inserted into a parent of any state, it needs to be checked which fields refer to the size, count or presence of the block itself or one of its ancestors. For each affected field function field, the following is done:
  \begin{itemize}
  \item \texttt{SizeOf} and \texttt{SummedSizeOf}: Add the newly inserted block's size to the current interpreted value of the field
  \item \texttt{PresenceOf}: Set the flag indicating the presence to the value indicated by the field function
  \item \texttt{CountOf}: Increment the current interpreted value of the field by 1
  \end{itemize}
}
{% Rationale
Required by \DesLink{dd:662}
}
{% Disadvantages
No disadvantages known
}
%%%% <-- DD %%%%

Finally, we need to think about how the insertion of bytes really happens. First of all, let us think about what we need to achieve after the insert:
\begin{itemize}
\item Each inserted data block (i.e. the actually inserted one and all its descendants) needs to have an associated \texttt{MediumOffset} which is the insertion offset for all of them at insert time, and gets updated to its final offset during a flush.
\item One or several inserts of concrete bytes must be scheduled in \COMPmedia{} in correct order.
\end{itemize}

One could come up with two distinct strategies to achieve these goals:
\begin{enumerate}
\item [\textbf{Option 1:}] \textbf{Insert bottom-up (fine-grained insertions)} - Only fields as leafs of the hierarchy are actually inserted one by one in correct order
\item [\textbf{Option 2:}] \textbf{Insert top-down (coarse-grained insertions)} - At the level of insertion (container, header, field etc.), all leaf fields are collected and their binary representations are merged to form larger chunks which are then inserted as needed.
\end{enumerate}

We can come up with pros and cons for both approaches:
\begin{longtable}{|p{0.2\textwidth}|p{0.35\textwidth}|p{0.35\textwidth}|}
	\hline
	\rowcolor[gray]{.9} & \textbf{Advantages} & \textbf{Disadvantages} \\
	\endhead
	\hline
        \textbf{Option 1:} Insert bottom- up (fine-grained insertions) & 
	$+$ Field offsets after flush need not be calculated, as \DesLink{dd:418b} takes effect & $-$ Offsets for parent data blocks need to be determined and assigned  \\
	\hline
         & 
	$+$ Merging of data and potential memory doubling not necessary & $-$ Probably imperformant writing, as a lot of small byte sequences (field-by-field) are written \\
	\hline \textbf{Option 2:} Insert top- down (coarse-grained insertions) & 
  $+$ Better write performance as we can write large chunks instead of only small ones & $-$ Offsets of child data blocks need to be determined and assigned \\
	\hline
         & 
	& $-$ Complex merging requiring tree traversal and most probably doubling of memory and copying of bytes \\
	\hline
\caption{Pros and cons of two insertion approaches}
\label{tab:InsertProCon}
\end{longtable}

This already indicates we have a winner:

%%%% DD --> %%%%
\DD{dd:684}
{% Title
Insertion uses \COMPmedia{}s insert scheduling, done for each field in field order
}
{% Short description
We choose \textbf{Option 1: Insert bottom-up (fine-grained insertions)}: When inserting a datablock into a parent, for each field contained in the data block (or for  itself, if it is a field), its binary value is inserted at the insertion offset using \COMPmedia{}s \texttt{insertData} method (see section \SectionLink{sec:ZugriffEinesTERMmedium}). Thereby the fields are visited in their specification order. The \texttt{MediumAction} instance returned by \texttt{insertData} is stored in each field.
}
{% Rationale
  There is no other reason for the existence of insert scheduling. Why do we persist only fields and no other types of datablocks? Clearly, a container and every other data block in the end consists of fields in a defined order as the leafs of the data block hierarchy. The fields contain interpreted values that can be converted to binary values. These binary values are then ultimately persisted on the medium. Determine parent offsets is much easier than determining child offsets as a parent data block always starts at the same offset as its first leaf field. If necessary, \COMPmedia{}'s flush can be changed to ensure small consecutive write actions are not written one by one but rather merged (up to reaching maximum read write block size) before being written.

  Doing it in field order is clearly indicated by \DesLink{dd:422}, as the insert offset for all new fields will be the same, their order must be clearly indicated by the call order of \texttt{insertData}. Doing it in another order thus would mess up the structure of the datablock, as fields are not persisted in the correct order as indicated in the data format specification.
}
{% Disadvantages
See table \ref{tab:InsertProCon}, disadvantes of option 1, but note the mitigations we noted down in the rationale.
}
%%%% <-- DD %%%%

A follow-up question is: Should we allow additional inserts in the middle of an already inserted data block? E.g. imagine you have inserted a container, but now after insertion you want to add another optional header to it. Is it allowed?

%%%% DD --> %%%%
\DD{dd:685}
{% Title
Insert into an already \texttt{INSERTED} ancestor is rejected
}
{% Short description
It is not allowed to insert a new data block into an already \texttt{INSERTED} ancestor.
}
{% Rationale
This reduces complexity, as otherwise one would need to first undo all inserts of fields that come behind the newly inserted child, and then reinsert to ensure they have the right order after a flush (compare \DesLink{dd:422}).
}
{% Disadvantages
Of course this is less convenient for the user, as he must ensure that such ``late'' insertions do not happen, but he only inserts fully assembled data blocks before a flush.
}
%%%% <-- DD %%%%

At first glance, it seems utterly inlikely that a user of \LibName{} would want to insert a data block larger than int size (i.e. 2.15 gigabytes). However, a data block may have theoretically long size. Thus, however unlikely this might be, we have to cover this case:
%%%% DD --> %%%%
\DD{dd:685b}
{% Title
Inserting a data block longer than max int size by multiple inserts in \COMPmedia{}
}
{% Short description
Inserting a data block being longer than maximum integer size is done by issuing multiple inserts in \COMPmedia{}. The datablock thus has to keep multiple \texttt{MediumAction}s.
}
{% Rationale
All \COMPmedia{} methods taking a size are using int as data type to ensure only single \texttt{ByteBuffer}s need to be handled, see section \SectionLink{sec:ZugriffEinesTERMmedium}. And as our data blocks could have max long size, we need to cover this case, too. Anyway, it might not be unlikely anymore that a user comes with huge data blocks, as RAMs also increase year by year.
}
{% Disadvantes
We have to manage multiple \texttt{MediumAction}s in a data block, probably leading to more complexity for undo operations and the like.
}
%%%% <-- DD %%%%


% -------------------------------------------------------------------------------------------------------

\subsubsection{Removing Data Blocks}%
\label{sec:RemovingDatablocks}%

Removal of data blocks can be designed in a very similar way to insertion.

\texttt{PERSISTED}, \texttt{MODIFIED}, \texttt{INSERTED} or \texttt{NEW} datablocks are removed in their parent or at the top-level of the medium, respectively. Here, one could argue that also having a remove method on the child itself could be valid, as a data block has at most one parent. But we go for the same approach as for insertion:
%%%% DD --> %%%%
\DD{dd:690}
{% Title
Remove methods for headers, footers and fields in their parent data block
}
{% Short description
To remove headers, footers or fields, there are corresponding insert methods that do the job in the corresponding datablock class. They take an index that must be valid.
}
{% Rationale
We are in line with insertion design and this is the way of least surprise. Having a remove method in the child would only confuse API users. Anyway, there are not so much commonalitities that a single method would make sense, i.e. removing of a header might differ from removal of a field.
}
{% Disadvantages
No disadvantages known
}
%%%% <-- DD %%%%

%%%% DD --> %%%%
\DD{dd:691}
{% Title
Remove methods for containers in container iterators
}
{% Short description
For both top-level as well as payload container iterators, there is a remove method in the iterator removing a container before the next container. That means to remove at a specific position, users first need to iterate to it. Likewise, the iterator's remove method can be designed to also the container remove method. 
}
{% Rationale
This is quite similar to the removal of headers, footers and fields.
}
{% Disadvantages
No disadvantages known
}
%%%% <-- DD %%%%

Payload is simply a mandatory datablock and thus cannot be removed:
%%%% DD --> %%%%
\DD{dd:692}
{% Title
No remove method for payload datablocks
}
{% Short description
Payload children cannot be removed
}
{% Rationale
There is always just exactly one payload in a container and it is mandatory.
}
{% Disadvantages
No disadvantages known
}
%%%% <-- DD %%%%

For the lists of children in parents, the very same design decision holds true as for insertions, see \DesLink{dd:679}, meaning that remove methods manage also removing the child from the internal lists, if necessary.

In \DesLink{dd:662} it was said that specification validity should be ensured as early as possible, for removals that means:
%%%% DD --> %%%%
\DD{dd:693}
{% Title
Specification validations during removal
}
{% Short description
The following checks against the specification are performed when removing a child, and if these checks fail, a runtime exception is thrown:
\begin{itemize}
\item Is the parent read-only?
\item Is the minimum parent size reached with this removal (e.g. is the parent fixed size)?
\item Is the minimum number of occurrences reached, e.g. is the data block present just once and not optional?
\end{itemize}
}
{% Rationale
In accordance to \DesLink{dd:662}
}
{% Disadvantages
No disadvantages known
}
%%%% <-- DD %%%%

If all these checks are done, further validations regarding the parent and child state are necessary:
%%%% DD --> %%%%
\DD{dd:694}
{% Title
Required datablock states
}
{% Short description
The datablock to remove as well as its parent must have one of the states \texttt{NEW}, \texttt{PERSISTED}, \texttt{MODIFIED} or \texttt{INSERTED}. That means neither the block to remove nor its parent must have been already removed. If one of these conditions is not met, a runtime exception is thrown.
}
{% Rationale
For all of these states, it would be a valid need for a user to remove a block. We could even be tolerant in accepting removes for already removed blocks.
}
{% Disadvantages
No disadvantages known
}
%%%% <-- DD %%%%

The next question regarding states is: How do states change after the remove?
%%%% DD --> %%%%
\DD{dd:695}
{% Title
State changes caused by a removal
}
{% Short description
If the parent block or the to-be-removed block is \texttt{NEW}, it remains \texttt{NEW}. If the removed block is \texttt{INSERTED}, it becomes \texttt{NEW}. Otherwise the removed block and all its descendants are changed into state \texttt{REMOVED}. The parent becomes \texttt{MODIFIED}, if it was \texttt{PERSISTED} before, otherwise it remains in its previous state.
}
{% Rationale
The states thus are quite conceivable.
}
{% Disadvantages
No disadvantages known
}
%%%% <-- DD %%%%

As described in \DesLink{dd:662}, we need to adress fields with field functions during removal. How this is done is described in the following design decision:
%%%% DD --> %%%%
\DD{dd:696}
{% Title
Changes in field functions fields during removals
}
{% Short description
  If a datablock is removed from a parent of any state, it needs to be checked which fields refer to the size, count or presence of the block itself or one of its ancestors. For each affected field function field, the following is done:
  \begin{itemize}
  \item \texttt{SizeOf} and \texttt{SummedSizeOf}: Subtract the removed block's size from the current interpreted value of the field
  \item \texttt{PresenceOf}: Set the flag indicating the absence to the value indicated by the field function
  \item \texttt{CountOf}: Decrement the current interpreted value of the field by 1
  \end{itemize}
}
{% Rationale
Required by \DesLink{dd:662}
}
{% Disadvantages
No disadvantages known
}
%%%% <-- DD %%%%

Finally, we need to think about how the removal of bytes really happens. For insertions, we said that it would be best to have one low-level \COMPmedia{} insert for each inserted field as these contain the bytes. For removals, we do it the other way round:

%%%% DD --> %%%%
\DD{dd:697}
{% Title
Remove uses \COMPmedia{}s remove scheduling, done for the actually removed data block
}
{% Short description
A remove is scheduled for the data block that is removed over its whole size.
}
{% Rationale
In contrast to insertions, we do not need to state which bytes to remove by having these, but you just can say \emph{how much} bytes to remove (see section \SectionLink{sec:ZugriffEinesTERMmedium}). It is more efficient to issue just one such remove instead of hierarchically traversing all fields and issueing separate removes. 
}
{% Disadvantages
No disadvantages known
}
%%%% <-- DD %%%%

Removes of children that are already removed are not possible (see \DesLink{dd:694}). However, at least it could happen that a user first removes a child, then the parent. This is allowed:

%%%% DD --> %%%%
\DD{dd:698}
{% Title
Removing an ancestor of an already \texttt{REMOVED} datablock is allowed
}
{% Short description
It is allowed to remove an ancestor of an already \texttt{REMOVED} block.
}
{% Rationale
Removing a region from the medium fully encompassing an already removed region is also an allowed use case in \COMPmedia{} and leads to the first remove being cancelled (see \DesLink{dd:424bb}).
}
{% Disadvantages
No disadvantages known
}
%%%% <-- DD %%%%

Quite in accordance with insertion (\DesLink{dd:685b}), we have to handle the case of removal of data blocks larger than int size:
%%%% DD --> %%%%
\DD{dd:698b}
{% Title
Removing a data block longer than max int size by multiple removes in \COMPmedia{}
}
{% Short description
Removing a data block being longer than maximum integer size is done by issuing multiple removes in \COMPmedia{}. The datablock thus has to keep multiple \texttt{MediumAction}s.
}
{% Rationale
All \COMPmedia{} methods taking a size are using int as data type to ensure only single \texttt{ByteBuffer}s need to be handled, see section \SectionLink{sec:ZugriffEinesTERMmedium}. And as our data blocks could have max long size, we need to cover this case, too. Anyway, it might not be unlikely anymore that a user comes with huge data blocks, as RAMs also increase year by year.
}
{% Disadvantes
We have to manage multiple \texttt{MediumAction}s in a data block, probably leading to more complexity for undo operations and the like.
}
%%%% <-- DD %%%%

% -------------------------------------------------------------------------------------------------------
\subsubsection{Modifying Datablocks}%
\label{sec:ModifyingDatablocks}%

Modifying data blocks mostly means modifying the value of a field. Clearly this is because all other data blocks are consisting of children, and these can be modified. Here we define the conditions for modifying field values.

For modification, the user can use the following:
%%%% DD --> %%%%
\DD{dd:705}
{% Title
Setter for setting the interpreted as well as binary value of a field
}
{% Short description
For fields, there is a setter for the binary value to directly set the bytes of the field as well as a setter for the interpreted value, i.e. ``human-readable'' value. Calling the interpreted value setter will immediately transform the value to bytes and vice versa.
}
{% Rationale
This is quite fair to allow both representations to be changed.
}
{% Disadvantes
No disadvantages known
}
%%%% <-- DD %%%%

Now let us clarify the state handling of modifications of fields:
%%%% DD --> %%%%
\DD{dd:706}
{% Title
Fields in all states but \texttt{REMOVED} can be modified  
}
{% Short description
The only state rejecting modification with a runtime exception is \texttt{REMOVED}.
}
{% Rationale
Modifying a field's value that is anyway getting removed makes no sense, and would lead to runtime exceptions because of \DesLink{dd:424a}.
}
{% Disadvantes
No disadvantages known
}
%%%% <-- DD %%%%

In line with that, let's cover the state changes:
%%%% DD --> %%%%
\DD{dd:707}
{% Title
State changes caused by a modification
}
{% Short description
If the parent block or the to-be-modified field is \texttt{NEW}, \texttt{MODIFIED} or \texttt{INSERTED}, both parent and field remain so. If the modified field and its parent are in state \texttt{PERSISTED}, both change into \texttt{MODIFIED} state.
}
{% Rationale
The states thus are quite conceivable.
}
{% Disadvantages
No disadvantages known
}
%%%% <-- DD %%%%

Field value changes of course also underly validation against specification:
%%%% DD --> %%%%
\DD{dd:708}
{% Title
Specification validations during field modification
}
{% Short description
The following checks against the specification are performed when modifying a field, and if these checks fail, a runtime exception is thrown:
\begin{itemize}
\item Is the field read-only?
\item Is the minimum or maximum field size reached with this change (e.g. is it fixed size)?
\item Is the field value one of the allowed enumerated values?
\item Can the field's interpreted value be converted to binary (and vice versa) without error?
\end{itemize}
}
{% Rationale
In accordance to \DesLink{dd:662}
}
{% Disadvantages
No disadvantages known
}
%%%% <-- DD %%%%

Quite the same way as for insertions and removals, we need to adress fields with field functions during modifications of a field. However, this has two aspects to it:
%%%% DD --> %%%%
\DD{dd:709}
{% Title
Changes in field functions fields during field modification
}
{% Short description
  If a field is modified and its size changes by this modification, it needs to be checked which fields refer to the size of the field itself or one of its ancestors. For each affected field function field, the following is done for \texttt{SizeOf} and \texttt{SummedSizeOf}: Subtract or add the field size change from the current interpreted value of the field.
}
{% Rationale
Required by \DesLink{dd:662}
}
{% Disadvantages
No disadvantages known
}
%%%% <-- DD %%%%

%%%% DD --> %%%%
\DD{dd:710}
{% Title
\texttt{CharacterEncodingOf} and \texttt{ByteOrderOf} fields lead to inverse changes
}
{% Short description
If a field defines a \texttt{CharacterEncodingOf} or \texttt{ByteOrderOf} field function, it can be modified itself to reflect a different character encoding or byte order. Of course, to ensure consistency of the data according to \DesLink{dd:662}, the affected target blocks might need to be changed then: They need to be reformatted to match the newly set character encoding or byte order.
}
{% Rationale
See \DesLink{dd:662}
}
{% Disadvantes
No disadvantages known
}
%%%% <-- DD %%%%

Finally, we have to think about how modifications of fields are brought to the medium:
%%%% DD --> %%%%
\DD{dd:711}
{% Title
Field modifications use \COMPmedia{}s replace scheduling
}
{% Short description
Changing the value of a field schedules a replace in \COMPmedia{} as long as the field is not \texttt{NEW} or \texttt{INSERTED}.
}
{% Rationale
Clearly the way to go as \texttt{PERSISTED} and \texttt{MODIFIED} fields clearly already are present on the medium, and thus modifying their value means to replace their previous value with something new. This also easily covers subsequent modificaitons of the same field: Simply issue the same replace multiple times, always specifying the old size as to be replaced and the new bytes (see \DesLink{dd:424a}).
}
{% Disadvantes
No disadvantages known
}
%%%% <-- DD %%%%

For \texttt{NEW} and \texttt{INSERTED} fields, it holds:

%%%% DD --> %%%%
\DD{dd:711b}
{% Title
For fields still \texttt{NEW}, no replace is scheduled yet, for \texttt{INSERTED} fields, another insert is scheduled
}
{% Short description
If a \texttt{NEW} field is modified, it means it anyway hasn't yet a scheduled \texttt{MediumAction}, so we also won't be scheduling any replace when modifying the field's value. For \texttt{INSERTED} fields, it holds that there is already a scheduled insert in accordance to \DesLink{dd:684}. This \texttt{MediumAction} needs to be undone, and instead a new insert with the new data needs to be scheduled.
}
{% Rationale
These two states are different than the others and thus require the special handling.
}
{% Disadvantes
No disadvantages known
}
%%%% <-- DD %%%%

As unlikely as it seems now, again we have to handle the case of field modifications where the new field size exceeds integer maximum value:
%%%% DD --> %%%%
\DD{dd:712}
{% Title
Modifying a field to longer than max int size by multiple replaces in \COMPmedia{}
}
{% Short description
Modifying a field being longer than maximum integer size is done by issuing multiple replaces in \COMPmedia{}. The datablock thus has to keep multiple \texttt{MediumAction}s.
}
{% Rationale
All \COMPmedia{} methods taking a size are using int as data type to ensure only single \texttt{ByteBuffer}s need to be handled, see section \SectionLink{sec:ZugriffEinesTERMmedium}. And as our data blocks could have max long size, we need to cover this case, too. Anyway, it might not be unlikely anymore that a user comes with huge data blocks, as RAMs also increase year by year.
}
{% Disadvantes
We have to manage multiple \texttt{MediumAction}s in a data block, probably leading to more complexity for undo operations and the like.
}
%%%% <-- DD %%%%

% -------------------------------------------------------------------------------------------------------
\subsubsection{Flushing}%
\label{sec:Flushing}%

Flushing all changes is unfortunately not exactly done by just calling flush of \COMPmedia{}.

First of all, we have to think of where the flush actually happens:
%%%% DD --> %%%%
\DD{dd:720}
{% Title
Writing of all changes done in top-level container iterator
}
{% Short description
The top-level container iterator offers a method to flush all previously made changes onto the medium. It is clear that this operation is not ACID as already pointed out in \DesLink{dd:663}.
}
{% Rationale
The top-level container iterator is the starting point of working with a medium and the logical place to have the flushing method(s).
}
{% Disadvantes
No disadvantages known
}
%%%% <-- DD %%%%

Flushing in \COMPdataPartManagement{} needs to do the following:
%%%% DD --> %%%%
\DD{dd:721}
{% Title
Steps of flushing
}
{% Short description
First of all, we call \texttt{MediumStore.flush()} to actually write all changes to the medium. Second, we set the states of all previously \texttt{INSERTED} or \texttt{MODIFIED} data blocks to \texttt{PERSISTED} again, and the states of all previously \texttt{REMOVED} data blocks to \texttt{NEW}.
}
{% Rationale
Flushing the store actually invalidates all \texttt{MediumAction}s (see \DesLink{dd:439f}) and finally determins the actual \texttt{MediumOffset}s of each data block (see \DesLink{dd:418b}). Also cleaning up the states is clearly indicated. The order of operations is as such to at least have some basic sanity in case of failure: If flushing fails in the middle, i.e. some changes have been written and others not, at least the \texttt{MediumAction} and \texttt{MediumOffset}s up to the point of failure are valid and these updates are present in the data blocks. Only their states would not be correct. Even ``retrying'' would then probably be possible with just a few lines of special handling.
}
{% Disadvantes
No disadvantages known
}
%%%% <-- DD %%%%

% -------------------------------------------------------------------------------------------------------
\subsubsection{Undo all Changes}%
\label{sec:UndoallChanges}%

According to \DesLink{dd:663}, it has to be possible to undo changes done previously using insertions, removals or modifications. This especially becomes interesting if multiple changes are done before a flush.

First of all, we have to think of where the ``undo all'' actually happens:
%%%% DD --> %%%%
\DD{dd:730}
{% Title
Undoing all changes is done in top-level container iterator
}
{% Short description
The top-level container iterator offers a method to undo all previously made changes. It is clear that this operation is not ACID as already pointed out in \DesLink{dd:663}.
}
{% Rationale
The top-level container iterator is the starting point of working with a medium and the logical place to have the undo all method(s).
}
{% Disadvantes
No disadvantages known
}
%%%% <-- DD %%%%

Now we not only need an undo all, but also the possibility to undo single changes with the following rules:
%%%% DD --> %%%%
\DD{dd:731}
{% Title
Undoing changes for \texttt{MODIFIED} and \texttt{INSERTED} data blocks
}
{% Short description
For \texttt{MODIFIED} and \texttt{INSERTED} data blocks, there is an undo operation that undoes the change in \COMPmedia{} for fields or traverses all field children for non-fields and calls undo. During this process, for all \texttt{MODIFIED} fields, the previous field's value is re-read from the medium to be restored in place. For all \texttt{INSERTED} fields, the values remain unchanged. For previously \texttt{MODIFIED} data blocks, their state is set to \texttt{PERSISTED}, for previously \texttt{INSERTED} data blocks, their state is set to \texttt{NEW}.
}
{% Rationale
  We have to somehow restore the initial state of the fields when undoing.
}
{% Disadvantes
No disadvantages known
}
%%%% <-- DD %%%%

%%%% DD --> %%%%
\DD{dd:732}
{% Title
Undoing changes for \texttt{REMOVED} data blocks
}
{% Short description
For \texttt{REMOVED} data blocks, the undo operation can only be called if the data blocks parent is not \texttt{REMOVED}, otherwise it fails. Then it undoes the \texttt{MediumAction} on the data block it is called and sets the states of all data blocks to \texttt{PERSISTED}.
}
{% Rationale
  We have to somehow restore the initial state of the fields when undoing.
}
{% Disadvantes
No disadvantages known
}
%%%% <-- DD %%%%

However, a bigger complexity arises when assuming multiple changes done on the same data block before the removal. We first have to state clearly:
%%%% DD --> %%%%
\DD{dd:733}
{% Title
Undo is rather a reset as it does only reset to the \emph{initial} state
}
{% Short description
When undoing, the operation does not restore the \emph{previous} state, but the \emph{initial} state of the data block. E.g. first inserting a new child, then removing it: Undo does not restore the child on first call and remove it again on the second. So undo uses no history or stack of any kind to behave like undo in an editor, e.g.
}
{% Rationale
Otherwise we would need to keep tracke of a change history of some sort for each data block which would make things overly complicated. It is also not very likely that users actually need incremental undo functionalities. Rather, they just need the possibility to reset the initial state as read from the medium. 
}
{% Disadvantes
No disadvantages known
}
%%%% <-- DD %%%%

Still, even if we now made our lives easier, there is one case that requires a re-read of fields again:
%%%% DD --> %%%%
\DD{dd:774}
{% Title
Undo removal of fields that were previously modified
}
{% Short description
If the user modifies a field previously read from the medium, the fields value (interpreted and binary) is changed to the new value. If the user than removes the field itself or its ancestor, it is set to \texttt{REMOVED}, still keeping the modified value. If the user now calls undo, the field would be \texttt{PERSISTED}, but with a value that does not match the fields actual value on the external medium. To mitigate this, whenever a \texttt{MODIFIED} field gets \texttt{REMOVED}, we re-read the fields actual value from the medium.
}
{% Rationale
This is the most efficient way to handle this situation. Another possibility would have been to ``simple'' re-read all field values from the medium once a user undoes a remove, but this is very inefficient.
}
{% Disadvantes
No disadvantages known
}
%%%% <-- DD %%%%

% =======================================================================================================
\subsection{Datablock Event Handling}%
\label{sec:DatablockEventHandling}%

We can see that insertion, removal or modification of a data block lead to a lot of things happening:
\begin{itemize}
\item State changes in the data block itself
\item State changes in its ancestors as well as descendants
\item Field function field updates
\end{itemize}

In addition, we need some way to know which data blocks were updated to be able to either flush or undo them all at once.

These things can all be handled by a single measure:
%%%% DD --> %%%%
\DD{dd:740}
{% Title
Event handling mechanism for processing data block changes
}
{% Short description
Data block changes emit events that are propagated to any interested party, i.e. those being registered for events. This includes ancestors, descendants as well as the container context and the top-level container iterator.
}
{% Rationale
Changes do not only have local effect in the data block itself but need to cause changes at some other places, too. Instead of directly couple all those places together, we opt for a decoupling via event handling: A set of listeners registers to the event emitter and is notified if something changes. The listener decides if the event is of interest and processes it correspondingly. This decouples emitters and receivers way better, yet even allowing extending receivers to add new functionality in future.
}
{% Disadvantes
Slight increase of complexity by introducing corresponding methods and classes for event handling
}
%%%% <-- DD %%%%

One question we should answer is: How do the events look like, i.e. which data is contained? We can state that all events are somehow related to data blocks. Thus we define:
%%%% DD --> %%%%
\DD{dd:741}
{% Title
Class \texttt{DataBlockEvent} contains the event type and causing data block
}
{% Short description
  We introduce a new class \texttt{DataBlockEvent} that contains:
  \begin{itemize}
  \item \textbf{The event type:} One of \texttt{INSERTED} (data block has been inserted), \texttt{PERSISTED} (data block has been persisted due to a flush), \texttt{REMOVED} (data block has been removed), \texttt{MODIFIED} (data block has been modified), \texttt{RESET} (data block has been reset, i.e. undone), \texttt{FLUSHED} (all changes where flushed), \texttt{RESET\_ALL} (all changes where reset, i.e. undone)
  \item \textbf{The event time stamp:} States the point in time the event occurred
  \item \textbf{The causing data block:} The data block that has caused the event, null for \texttt{FLUSHED} and \texttt{RESET\_ALL} 
  \end{itemize}
}
{% Rationale
Except flush and reset, we only have events related to data blocks. The receiver needs to know who caused the event to do the right thing, as well as what exactly has happened with the block.
}
{% Disadvantes
No disadvantages known
}
%%%% <-- DD %%%%

Another question is: How are events exactly propagated?
%%%% DD --> %%%%
\DD{dd:742}
{% Title
Broadcast bus  with all data blocks, container contexts and the top-level iterator as listeners
}
{% Short description
A broadcast mechanism with a bus is used, all data blocks and container contexts as well as top-level iterators are registered as listeners. Every event causes a broadcast on the bus to all other receivers. They filter the event to decide if they need to do something and otherwise ignore it.
}
{% Rationale
We have a central point for registration of listeners, notification as well as monitoring. Broadcasts are fine because all of the events are relevant for multiple, sometimes even all listeners. It is also more flexible and easier to extend than rigid solutions. 
}
{% Disadvantes
Some listeners are unnecessarily informed about an event they do not care about. Second, it is not clear how we ensure that events are not repeated or lead to too much follow-up events.
}
%%%% <-- DD %%%%

Finally, we shall ensure that an event does not lead to a flood of follow-up events:
%%%% DD --> %%%%
\DD{dd:743}
{% Title
A state change in a data block alone is not enough for an event
}
{% Short description
Whenever a child is \texttt{MODIFIED}, \texttt{INSERTED} or \texttt{REMOVED}, a corresponding event is sent that lead to corresponding changes of states also in ancestors and descendants. However, for these indirect changes, no additional event is sent to the bus.
}
{% Rationale
Avoids flooding of bus and listeners with secondary events and thus excludes the need for special case handling. Also, this might reduce the possibility of endless event loops.
}
{% Disadvantes
No disadvantages known
}
%%%% <-- DD %%%%

% =======================================================================================================
\subsection{API Design}%
\label{sec:APIDesign}%

\small
\begin{landscape}
\begin{longtable}{|p{0.3\linewidth}|p{0.65\linewidth}|}
\hline
\rowcolor[gray]{.9}\textbf{Operation} & \textbf{Description} \\
\endhead
\hline
\texttt{DataBlock}\linebreak\texttt{.getId()} & Gets the \texttt{DataBlockId} of the \texttt{DataBlock} \\
\hline
\texttt{DataBlock}\linebreak\texttt{.getSequenceNumber()} & Gets sequence number, which is the occurrence index in its parent \\
\hline
\texttt{DataBlock}\linebreak\texttt{.getOffset()} & Returns the \texttt{MediumOffset} this \texttt{DataBlock} is currently located on the external medium or at least scheduled to be inserted at this position. Returns null if the \texttt{DataBlock} is still \texttt{NEW} or \texttt{INSERTED}. \\
\hline
\texttt{DataBlock}\linebreak\texttt{.getSize()} & Returns the total size of the \texttt{DataBlock} in bytes, if currently known, or \texttt{DataBlockDescription.UNDEFINED} otherwise \\
\hline
\texttt{DataBlock}\linebreak\texttt{.getState()} & Gets the current state of the \texttt{DataBlock}. \\
\hline
\texttt{DataBlock}\linebreak\texttt{.getBytes(ofs, n)} & Gets the DataBlock's bytes starting at the given global offset with the given length. Both offset and length must be within the actual start offset plus total size range. The bytes are only returned if the \texttt{DataBlock} is in state \texttt{PERSISTED}, \texttt{REMOVED} or \texttt{MODIFIED}, and then taken from the external medium. For fields, this method also returns bytes for other states, which are the bytes actually set by the user. \\
\hline
\texttt{DataBlock}\linebreak\texttt{.getContainerContext()} & Returns the \texttt{ContainerContext} of the next higher ancestor \texttt{Container} this \texttt{DataBlock} is associated with (either as it is the \texttt{Container} itself or its descendant). Returns null for \texttt{NEW} \texttt{DataBlock}s. \\
\hline
\texttt{DataBlock}\linebreak\texttt{.getParent()} & Returns the direct parent of the \texttt{DataBlock} if it is not \texttt{NEW}. Otherwise it returns null. For top-level \texttt{DataBlock}s, this method also returns null. \\
\hline
\texttt{DataBlock}\linebreak\texttt{.initParent(DataBlock)} & Initializes the parent \texttt{DataBlock}, i.e. must only be called once. \\
\hline
\texttt{DataBlock}\linebreak\texttt{.initContainerContext(ContainerContext)} & Initializes the \texttt{ContainerContext}, i.e. must only be called once. \\
\hline
\texttt{DataBlock}\linebreak\texttt{.reset()} & Resets all changes within this \texttt{DataBlock} itself and all its descendends, i.e. resets any modifications, removals and inserts. \\
\hline
\caption{Operations of the \texttt{DataBlock} interface}
\label{tab:DBOpsDatablock}
\end{longtable}
\end{landscape}
\normalsize

\small
\begin{landscape}
\begin{longtable}{|p{0.3\linewidth}|p{0.65\linewidth}|}
\hline
\rowcolor[gray]{.9}\textbf{Operation} & \textbf{Description} \\
\endhead
\hline
\texttt{ContainerIterator}\linebreak\texttt{.hasNext()} & Tells whether there is a next \texttt{Container} available that can be retrieved using \texttt{next()}. \\
\hline
\texttt{ContainerIterator}\linebreak\texttt{.next()} & Returns the next \texttt{Container} instance if available or throws a runtime exception if not. It thus also advances the iterator. \\
\hline
\texttt{ContainerIterator}\linebreak\texttt{.remove()}, \texttt{ContainerIterator.removeContainer()} & Schedules the current \texttt{Container} for removal. If done before any calls to \texttt{next()}, this removes the first \texttt{Container}, if any. If there is no next \texttt{Container}, a runtime exception is thrown. \\
\hline
\texttt{ContainerIterator}\linebreak\texttt{.insertContainer(Container)} & Inserts a \texttt{NEW} \texttt{Container} in front of the current \texttt{Container}. This method is allowed to be called even if \texttt{hasNext()} returns false in order to append a new \texttt{Container} at the end of the current payload or medium. \\
\hline
\caption{Operations of the \texttt{ContainerIterator} interface}
\label{tab:DBOpsContainerIterator}
\end{longtable}
\end{landscape}
\normalsize

\small
\begin{landscape}
\begin{longtable}{|p{0.3\linewidth}|p{0.65\linewidth}|}
\hline
\rowcolor[gray]{.9}\textbf{Operation} & \textbf{Description} \\
\endhead
\hline
\texttt{TopLevelContainerIterator}\linebreak\texttt{.close()} & Closes the use of the iterator for top-level container retrieval and writing. Once this is done, the instance must not be used anymore. It also closes the access to the underlying medium and cleans up any further resources. \\
\hline
\texttt{TopLevelContainerIterator}\linebreak\texttt{.writeAllChanges()} & Flushes all previously made changes (removals, inserts and modifications) to the external medium. \\
\hline
\texttt{TopLevelContainerIterator}\linebreak\texttt{.resetAllChanges()} & Resets all previously made changes (removals, inserts and modifications) as if they never happened. \\
\hline
\caption{Operations of the \texttt{TopLevelContainerIterator} interface}
\label{tab:DBOpsTopLevelContainerIterator}
\end{longtable}
\end{landscape}
\normalsize

\small
\begin{landscape}
\begin{longtable}{|p{0.3\linewidth}|p{0.65\linewidth}|}
\hline
\rowcolor[gray]{.9}\textbf{Operation} & \textbf{Description} \\
\endhead
\hline
\texttt{ContainerContext}\linebreak\texttt{.getDataFormatSpecification()} & Returns the DataFormatSpecification this context belongs to. \\
\hline
\texttt{ContainerContext}\linebreak\texttt{.getContainer()} & Returns the \texttt{Container} this context belongs to. \\
\hline
\texttt{ContainerContext}\linebreak\texttt{.getParentContainerContext()} & Returns the parent \texttt{ContainerContext} or null if it refers to a top-level \texttt{Container}. \\
\hline
\texttt{ContainerContext}\linebreak\texttt{.addFieldFunctions(Field)} & During reading: Adds all field functions of a given field (if any) to the \texttt{ContainerContext} \\
\hline
\texttt{ContainerContext}\linebreak\texttt{.getSizeOf(DataBlockId, int)} & Tries to determine the size of the indicated \texttt{DataBlockId} with the given sequence number or \texttt{DataBlockDescription.UNDEFINED} if it cannot be determined.\\
\hline
\texttt{ContainerContext}\linebreak\texttt{.getOccurrencesOf(DataBlockId)} & Tries to determine the number of occurrences of the indicated \texttt{DataBlockId}  or \texttt{DataBlockDescription.UNDEFINED} if it cannot be determined. \\
\hline
\texttt{ContainerContext}\linebreak\texttt{.getByteOrderOf(DataBlockId, int)} & Tries to determine the byte order of the indicated \texttt{DataBlockId}  with the given sequence number or \texttt{null} if it cannot be determined. \\
\hline
\texttt{ContainerContext}\linebreak\texttt{.getCharacterEncodingOf(DataBlockId, int)} & Tries to determine the character encoding of the indicated \texttt{DataBlockId}  with the given sequence number or \texttt{null} if it cannot be determined. \\
\hline
\caption{Operations of the \texttt{ContainerContext} interface}
\label{tab:DBOpsContainerContext}
\end{longtable}
\end{landscape}
\normalsize

\small
\begin{landscape}
\begin{longtable}{|p{0.3\linewidth}|p{0.65\linewidth}|}
\hline
\rowcolor[gray]{.9}\textbf{Operation} & \textbf{Description} \\
\endhead
\hline
\texttt{Container}\linebreak\texttt{.getHeaders()} & Returns the \texttt{Header}s of this \texttt{Container}. Unmodifiable list. \\
\hline
\texttt{Container}\linebreak\texttt{.getPayload()} & Returns the \texttt{Payload} of this \texttt{Container}. \\
\hline
\texttt{Container}\linebreak\texttt{.getFooters()} & Returns the \texttt{Footer}s of this \texttt{Container}. Unmodifiable list. \\
\hline
\texttt{Container}\linebreak\texttt{.setPayload(Payload)} & Sets the \texttt{Payload} of this \texttt{Container}. Must be a \texttt{NEW} \texttt{Payload} that actually belongs to this \texttt{Container}. The previous \texttt{Payload} is replaced by the new one. \\
\hline
\texttt{Container}\linebreak\texttt{.insertHeader(int, Header)} & Inserts a \texttt{NEW} \texttt{Header} into this \texttt{Container}. The given index must be valid. Must be a \texttt{NEW} \texttt{Header} that actually belongs to this \texttt{Container} and must not exceed the maximum occurrences of this header type. \\
\hline
\texttt{Container}\linebreak\texttt{.insertFooter(int, Footer)} & Inserts a \texttt{NEW} \texttt{Footer} into this \texttt{Container}. The given index must be valid. Must be a \texttt{NEW} \texttt{Footer} that actually belongs to this \texttt{Container} and must not exceed the maximum occurrences of this header type. \\
\hline
\caption{Operations of the \texttt{Container} interface}
\label{tab:DBOpsContainer}
\end{longtable}
\end{landscape}
\normalsize

\small
\begin{landscape}
\begin{longtable}{|p{0.3\linewidth}|p{0.65\linewidth}|}
\hline
\rowcolor[gray]{.9}\textbf{Operation} & \textbf{Description} \\
\endhead
\hline
\texttt{FieldSequence}\linebreak\texttt{.getFields()} & Returns the \texttt{Field}s of this \texttt{FieldSequence}. Unmodifiable list. \\
\hline
\texttt{FieldSequence}\linebreak\texttt{.insertField(int, Field)} & Inserts a \texttt{NEW} \texttt{Field} into this \texttt{FieldSequence}. The given index must be valid. Must be a \texttt{NEW} \texttt{Field} that actually belongs to this \texttt{FieldSequence} and must not exceed the maximum occurrences of this header type. \\
\hline
\caption{Operations of the \texttt{FieldSequence} interface, thus also for \texttt{FieldBasedPayload}, \texttt{Header} and \texttt{Footer}}
\label{tab:DBOpsFieldSequence}
\end{longtable}
\end{landscape}
\normalsize

\small
\begin{landscape}
\begin{longtable}{|p{0.35\linewidth}|p{0.6\linewidth}|}
\hline
\rowcolor[gray]{.9}\textbf{Operation} & \textbf{Description} \\
\endhead
\hline
\texttt{Field}\linebreak\texttt{.getInterpretedValue()} & Returns the interpreted value of the \texttt{Field}. \\
\hline
\texttt{Field}\linebreak\texttt{.getBinaryValue()} & Returns the binary value of the field as \texttt{ByteBuffer}. \\
\hline
\texttt{Field}\linebreak\texttt{.setInterpretedValue(T)} & Sets a new interpreted value for the Field. Must be a valid value and must not exceed the field's maximum size. \\
\hline
\texttt{Field}\linebreak\texttt{.setBinaryValue(ByteBuffer)} & Sets a new binary value for the field. Must be a valid value and must not exceed the field's maximum size. \\
\hline
\caption{Operations of the \texttt{Field} interface}
\label{tab:DBOpsField}
\end{longtable}
\end{landscape}
\normalsize

\small
\begin{landscape}
\begin{longtable}{|p{0.3\linewidth}|p{0.65\linewidth}|}
\hline
\rowcolor[gray]{.9}\textbf{Operation} & \textbf{Description} \\
\endhead
\hline
\texttt{ContainerBasedPayload}\linebreak\texttt{.getContainerIterator()} & Returns the \texttt{ContainerIterator} for iterating the child containers. \\
\hline
\caption{Operations of the \texttt{ContainerBasedPayload} interface}
\label{tab:DBOpsContainerBasedPayload}
\end{longtable}
\end{landscape}
\normalsize

\small
\begin{landscape}
\begin{longtable}{|p{0.35\linewidth}|p{0.6\linewidth}|}
\hline
\rowcolor[gray]{.9}\textbf{Operation} & \textbf{Description} \\
\endhead
\hline
\texttt{SizeProvider}\linebreak\texttt{.getSizeOf(DataBlockId, int, ContainerContext)} & Determins a custom size of the given \texttt{DataBlockId} and sequence number or \texttt{DataBlockDescription.UNDEFINED} if it cannot be determined. \\
\hline
\texttt{CountProvider}\linebreak\texttt{.getCountOf(DataBlockId, ContainerContext)} & Determins a custom count of the given \texttt{DataBlockId} or \texttt{DataBlockDescription.UNDEFINED} if it cannot be determined. \\
\hline
\caption{Operations of the \texttt{CountProvider} and \texttt{SizeProvider} interfaces}
\label{tab:DBOpsCountProvider}
\end{longtable}
\end{landscape}
\normalsize

\small
\begin{landscape}
\begin{longtable}{|p{0.3\linewidth}|p{0.65\linewidth}|}
\hline
\rowcolor[gray]{.9}\textbf{Operation} & \textbf{Description} \\
\endhead
\hline
\texttt{DataBlockEventBus}\linebreak\texttt{.registerListener(DataBlockEventListener)} & Registers a new \texttt{DataBlockEventListener}. \\
\hline
\texttt{DataBlockEventBus}\linebreak\texttt{.publishEvent(DataBlockEvent)} & Notifies all currently registered \texttt{DataBlockEventListener}s that a new event has occurred. The order of notification is undefined and listeners must not rely on it. \\
\hline
\texttt{DataBlockEventListener}\linebreak\texttt{.dataBlockEventOccurred(DataBlockEvent)} & Is called once whenever a new \texttt{DataBlockEvent} has occurred on the \texttt{DataBlockEventBus}. \\
\hline
\caption{Operations of the \texttt{DataBlockEventBus} and \texttt{DataBlockEventListener} interfaces}
\label{tab:DBOpsDataBlockEventBus}
\end{longtable}
\end{landscape}
\normalsize


\small
\begin{landscape}
\begin{longtable}{|p{0.3\linewidth}|p{0.65\linewidth}|}
\hline
\rowcolor[gray]{.9}\textbf{Operation} & \textbf{Description} \\
\endhead
\hline
\texttt{DataBlockFactory.}\texttt{createPersistedContainer(DataBlockId, int, MediumOffset, ContainerContext)} &  \\
\hline
\texttt{DataBlockFactory.}\texttt{createPersistedHeader(DataBlockId, int, MediumOffset, ContainerContext)} &  \\
\hline
\texttt{DataBlockFactory.}\texttt{createPersistedFooter(DataBlockId, int, MediumOffset, ContainerContext)} &  \\
\hline
\texttt{DataBlockFactory.}\texttt{createPersistedPayload(DataBlockId, int, MediumOffset, ContainerContext, long)} &  \\
\hline
\texttt{DataBlockFactory.}\texttt{createPersistedField(DataBlockId, int, MediumOffset, ContainerContext, ByteBuffer)} &  \\
\hline
\texttt{DataBlockFactory.}\texttt{createNewContainer(DataBlockId)} &  \\
\hline
\texttt{DataBlockFactory.}\texttt{createNewHeader(DataBlockId)} &  \\
\hline
\texttt{DataBlockFactory.}\texttt{createNewFooter(DataBlockId)} &  \\
\hline
\texttt{DataBlockFactory.}\texttt{createNewPayload(DataBlockId)} &  \\
\hline
\texttt{DataBlockFactory.}\texttt{createNewField(DataBlockId, T)} &  \\
\hline
\caption{Operations of the \texttt{DataBlockFactory} interface}
\label{tab:DBOpsDataBlockFactory}
\end{longtable}
\end{landscape}
\normalsize

TODO: DataBlockAccessor + DataBlockService

%###############################################################################################
%###############################################################################################
%
%		File end
%
%###############################################################################################
%###############################################################################################